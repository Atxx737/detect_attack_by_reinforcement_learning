{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6750,"status":"ok","timestamp":1685622332901,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"FUs6Oj1Xvgyz"},"outputs":[],"source":["import numpy as np\n","import random\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import RMSprop\n","from collections import deque \n","from tensorflow import gather_nd\n","from tensorflow.keras.losses import mean_squared_error \n","import pandas as pd"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102137,"status":"ok","timestamp":1685622326155,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"8f-bQUBdvgyy","outputId":"072377f3-e2f8-4234-dfa6-a30dc7a986b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# # Load the Drive helper and mount\n","# from google.colab import drive\n","\n","# # This will prompt for authorization.\n","# drive.mount('/content/drive')\n","\n","# TRAIN_PATH='/content/drive/MyDrive/UIT/HocKy8/KLTN/detect_attack_by_reinforcement_learning/data/matrix1/normal/train-normal.csv'\n","# dataset = pd.read_csv(TRAIN_PATH)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n"]}],"source":["TRAIN_PATH='../data/matrix1/normal/train-normal.csv'\n","dataset = pd.read_csv(TRAIN_PATH)\n","# print(type(dataset))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":382,"status":"ok","timestamp":1685622507188,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"H_BcnBTMvgyz"},"outputs":[],"source":["class DeepQLearning:\n","    \n","    ###########################################################################\n","    #   START - __init__ function\n","    ###########################################################################\n","    # INPUTS: \n","    # env - Cart Pole environment\n","    # gamma - discount rate\n","    # epsilon - parameter for epsilon-greedy approach\n","    # numberEpisodes - total number of simulation episodes\n","    \n","            \n","    def __init__(self,data,state_size,action_size,gamma,epsilon,batch_size):\n","    \n","        # self.env=env\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.numberEpisodes = 10\n","        self.data = data\n","        # state dimension\n","        # self.stateDimension=4\n","        self.state_size = state_size\n","\n","        # action dimension\n","        # self.actionDimension=2 # right or left\n","        self.action_size = action_size # attack or not\n","\n","        # this is the maximum size of the replay buffer\n","        self.replayBufferSize = 300\n","        # this is the size of the training batch that is randomly sampled from the replay buffer\n","        self.batchReplayBufferSize = batch_size\n","        \n","        # number of training episodes it takes to update the target network parameters\n","        # that is, every updateTargetNetworkPeriod we update the target network parameters\n","        self.updateTargetNetworkPeriod = 100\n","        \n","        # this is the counter for updating the target network \n","        # if this counter exceeds (updateTargetNetworkPeriod-1) we update the network \n","        # parameters and reset the counter to zero, this process is repeated until the end of the training process\n","        self.counterUpdateTargetNetwork = 0\n","        \n","        # this sum is used to store the sum of rewards obtained during each training episode\n","        self.sumRewardsEpisode = []\n","        \n","        # replay buffer\n","        self.replayBuffer = deque(maxlen=self.replayBufferSize)\n","        \n","        # this is the main network\n","        # create network\n","        self.mainNetwork = self.createNetwork()\n","        \n","        # this is the target network\n","        # create network\n","        self.targetNetwork = self.createNetwork()\n","        \n","        # copy the initial weights to targetNetwork\n","        self.targetNetwork.set_weights(self.mainNetwork.get_weights())\n","        \n","        # this list is used in the cost function to select certain entries of the \n","        # predicted and true sample matrices in order to form the loss\n","        self.actionsAppend = []\n","    \n","    ###########################################################################\n","    #   END - __init__ function\n","    ###########################################################################\n","    \n","    ###########################################################################\n","    # START - function for defining the loss (cost) function\n","    # INPUTS: \n","    #\n","    # y_true - matrix of dimension (self.batchReplayBufferSize,2) - this is the target \n","    # y_pred - matrix of dimension (self.batchReplayBufferSize,2) - this is predicted by the network\n","    # \n","    # - this function will select certain row entries from y_true and y_pred to form the output \n","    # the selection is performed on the basis of the action indices in the list  self.actionsAppend\n","    # - this function is used in createNetwork(self) to create the network\n","    #\n","    # OUTPUT: \n","    #    \n","    # - loss - watch out here, this is a vector of (self.batchReplayBufferSize,1), \n","    # with each entry being the squared error between the entries of y_true and y_pred\n","    # later on, the tensor flow will compute the scalar out of this vector (mean squared error)\n","    ###########################################################################    \n","    \n","    def my_loss_fn(self,y_true, y_pred):\n","        \n","        s1,s2=y_true.shape\n","        #print(s1,s2)\n","        \n","        # this matrix defines indices of a set of entries that we want to \n","        # extract from y_true and y_pred\n","        # s2=2\n","        # s1=self.batchReplayBufferSize\n","        indices=np.zeros(shape=(s1,s2))\n","        indices[:,0]=np.arange(s1)\n","        indices[:,1]=self.actionsAppend\n","        \n","        # gather_nd and mean_squared_error are TensorFlow functions\n","        loss = mean_squared_error(gather_nd(y_true,indices=indices.astype(int)), gather_nd(y_pred,indices=indices.astype(int)))\n","        #print(loss)\n","        return loss    \n","    ###########################################################################\n","    #   END - of function my_loss_fn\n","    ###########################################################################\n","    \n","    \n","    ###########################################################################\n","    #   START - function createNetwork()\n","    # this function creates the network\n","    ###########################################################################\n","    \n","    # create a neural network\n","    def  createNetwork(self):\n","        model=Sequential()\n","        # model.add(Dense(128,input_dim=self.stateDimension,activation='relu'))\n","        model.add(Dense(128,input_dim=self.state_size,activation='relu'))\n","        model.add(Dense(56,activation='relu'))\n","        # model.add(Dense(self.actionDimension,activation='linear'))\n","        model.add(Dense(self.action_size,activation='linear'))\n","        # compile the network with the custom loss defined in my_loss_fn\n","        model.compile(optimizer = RMSprop(), loss = self.my_loss_fn, metrics = ['accuracy'])\n","        return model\n","    ###########################################################################\n","    #   END - function createNetwork()\n","    ###########################################################################\n","            \n","    ###########################################################################\n","    #   START - function trainingEpisodes()\n","    #   - this function simulates the episodes and calls the training function \n","    #   - trainNetwork()\n","    ###########################################################################\n","\n","    def trainingEpisodes(self): \n","        \n","        # here we loop through the episodes = mini-batch size\n","        for indexEpisode in range(self.numberEpisodes):\n","            \n","            # list that stores rewards per episode - this is necessary for keeping track of convergence \n","            rewardsEpisode=[]\n","                       \n","            print(\"Simulating episode {}\".format(indexEpisode))\n","            \n","            # reset the environment at the beginning of every episode\n","            (currentState,_)=self.reset()\n","            # (currentState,_)=self.env.reset()\n","                      \n","            # here we step from one state to another\n","            # this will loop until a terminal state is reached\n","            # terminalState=False\n","            # while not terminalState:\n","\n","            for indexSample in range (len(self.data)):\n","                                      \n","                # select an action on the basis of the current state, denoted by currentState\n","                action = self.selectAction(currentState,indexEpisode)\n","                \n","                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n","                # (nextState, reward, terminalState,_,_) = self.env.step(action)       \n","                (nextState, reward, terminalState,_,_) = self.step(action,indexSample)       \n","                rewardsEpisode.append(reward)\n","         \n","                # add current state, action, reward, next state, and terminal flag to the replay buffer\n","                self.replayBuffer.append((currentState,action,reward,nextState,terminalState))\n","                \n","                # train network\n","                self.trainNetwork()\n","                \n","                # set the current state for the next step\n","                currentState=nextState\n","            \n","            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n","            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n","    ###########################################################################\n","    #   END - function trainingEpisodes()\n","    ###########################################################################\n","            \n","       \n","    ###########################################################################\n","    #    START - function for selecting an action: epsilon-greedy approach\n","    ###########################################################################\n","    # this function selects an action on the basis of the current state \n","    # INPUTS: \n","    # state - state for which to compute the action\n","    # index - index of the current episode\n","    def selectAction(self,state,index):\n","        import numpy as np\n","        \n","        # first index episodes we select completely random actions to have enough exploration\n","        # change this\n","        if index<1:\n","            return np.random.choice(self.action_size)   \n","            \n","        # Returns a random real number in the half-open interval [0.0, 1.0)\n","        # this number is used for the epsilon greedy approach\n","        randomNumber=np.random.random()\n","        \n","        # after index episodes, we slowly start to decrease the epsilon parameter\n","        if index>5:\n","            self.epsilon=0.999*self.epsilon\n","        \n","        # if this condition is satisfied, we are exploring, that is, we select random actions\n","        if randomNumber < self.epsilon:\n","            # returns a random action selected from: 0,1,...,actionNumber-1\n","            return np.random.choice(self.action_size)            \n","        \n","        # otherwise, we are selecting greedy actions\n","        else:\n","            # we return the index where Qvalues[state,:] has the max value\n","            # that is, since the index denotes an action, we select greedy actions\n","                       \n","            # Qvalues=self.mainNetwork.predict(state.reshape(1,4))\n","            Qvalues=self.mainNetwork.predict(state.reshape(1,self.state_size))\n","          \n","            return np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n","            # here we need to return the minimum index since it can happen\n","            # that there are several identical maximal entries, for example \n","            # import numpy as np\n","            # a=[0,1,1,0]\n","            # np.where(a==np.max(a))\n","            # this will return [1,2], but we only need a single index\n","            # that is why we need to have np.random.choice(np.where(a==np.max(a))[0])\n","            # note that zero has to be added here since np.where() returns a tuple\n","    ###########################################################################\n","    #    END - function selecting an action: epsilon-greedy approach\n","    ###########################################################################\n","    \n","    ###########################################################################\n","    #    START - function trainNetwork() - this function trains the network\n","    ###########################################################################\n","    \n","    def trainNetwork(self):\n","\n","        # if the replay buffer has at least batchReplayBufferSize elements,\n","        # then train the model \n","        # otherwise wait until the size of the elements exceeds batchReplayBufferSize\n","        if (len(self.replayBuffer)>self.batchReplayBufferSize):\n","            \n","\n","            # sample a batch from the replay buffer\n","            randomSampleBatch=random.sample(self.replayBuffer, self.batchReplayBufferSize)\n","            \n","            # here we form current state batch \n","            # and next state batch\n","            # they are used as inputs for prediction\n","            # currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))\n","            # nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))    \n","            currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,self.state_size))\n","            nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,self.state_size))            \n","            # this will enumerate the tuple entries of the randomSampleBatch\n","            # index will loop through the number of tuples\n","            for index,tupleS in enumerate(randomSampleBatch):\n","                # first entry of the tuple is the current state\n","                # currentStateBatch[index,:]=tupleS[0]\n","                currentStateBatch[index, :] = np.reshape(tupleS[0], (1, self.state_size))\n","                # fourth entry of the tuple is the next state\n","                # nextStateBatch[index,:]=tupleS[3]\n","                nextStateBatch[index, :] = np.reshape(tupleS[3], (1, self.state_size))\n","            \n","            # here, use the target network to predict Q-values \n","            QnextStateTargetNetwork=self.targetNetwork.predict(nextStateBatch)\n","            # here, use the main network to predict Q-values \n","            QcurrentStateMainNetwork=self.mainNetwork.predict(currentStateBatch)\n","            \n","            # now, we form batches for training\n","            # input for training\n","            inputNetwork=currentStateBatch\n","            # output for training\n","            outputNetwork=np.zeros(shape=(self.batchReplayBufferSize,2))\n","            \n","            # this list will contain the actions that are selected from the batch \n","            # this list is used in my_loss_fn to define the loss-function\n","            self.actionsAppend=[]            \n","            for index,(currentState,action,reward,nextState,terminated) in enumerate(randomSampleBatch):\n","                \n","                # if the next state is the terminal state\n","                if terminated:\n","                    y=reward                  \n","                # if the next state if not the terminal state    \n","                else:\n","                    y=reward+self.gamma*np.max(QnextStateTargetNetwork[index])\n","                \n","                # this is necessary for defining the cost function\n","                self.actionsAppend.append(action)\n","                \n","                # this actually does not matter since we do not use all the entries in the cost function\n","                outputNetwork[index]=QcurrentStateMainNetwork[index]\n","                # this is what matters\n","                outputNetwork[index,action]=y\n","            \n","            # here, we train the network\n","            self.mainNetwork.fit(inputNetwork,outputNetwork,batch_size = self.batchReplayBufferSize, verbose=0,epochs=100)     \n","            \n","            # after updateTargetNetworkPeriod training sessions, update the coefficients \n","            # of the target network\n","            # increase the counter for training the target network\n","            self.counterUpdateTargetNetwork+=1  \n","            if (self.counterUpdateTargetNetwork>(self.updateTargetNetworkPeriod-1)):\n","                # copy the weights to targetNetwork\n","                self.targetNetwork.set_weights(self.mainNetwork.get_weights())        \n","                print(\"Target network updated!\")\n","                print(\"Counter value {}\".format(self.counterUpdateTargetNetwork))\n","                # reset the counter\n","                self.counterUpdateTargetNetwork=0\n","    ###########################################################################\n","    #    END - function trainNetwork() \n","    ###########################################################################     \n","\n","    ###########################################################################\n","    # START - function for defining the reset function for ENV \n","    # reset the environment at the beginning of every episode\n","    ###########################################################################                \n","            \n","    def step(self,action,index):\n","        \n","        # super().reset(seed=seed)\n","        # Note that if you use custom reset bounds, it may lead to out-of-bound\n","        # state/observations.\n","        \n","        if action == self.data.iloc[index].values[-1]: \n","            reward = 1\n","        else:  \n","            reward = -1\n","\n","        next_state = self.data.iloc[index + 1].values[:-1]\n","        next_state = np.reshape(next_state, [1, self.state_size])\n","\n","        if index == (len(self.data)-1):\n","            terminated = True\n","        terminated = False\n","        self.state = next_state\n","\n","        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n","\n","    ###########################################################################\n","    #    END - function for defining the reset function for ENV \n","    ###########################################################################                \n","                \n","    ###########################################################################\n","    # START - function for defining the reset function for ENV \n","    # reset the environment at the beginning of every episode\n","    ###########################################################################                \n","            \n","    def reset(self):\n","      \n","        # super().reset(seed=seed)\n","        # Note that if you use custom reset bounds, it may lead to out-of-bound\n","        # state/observations.\n","        # self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n","\n","        #random index in mini-batch\n","        start_index = np.random.randint(0, len(self.data) - 1)\n","\n","        self.steps_beyond_terminated = None\n","        self.state =  self.data.iloc[start_index].values[:-1]\n","        # self.label =  self.data.iloc[start_index].values[-1]\n","\n","        return np.array(self.state, dtype=np.float32), {}\n","\n","    ###########################################################################\n","    #    END - function for defining the reset function for ENV \n","    ###########################################################################        \n","            "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685622336822,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"rzSiskhpvgy0"},"outputs":[],"source":["# select the parameters\n","gamma=1\n","# probability parameter for the epsilon-greedy approach\n","epsilon=0.1\n","# number of training episodes\n","# NOTE HERE THAT AFTER CERTAIN NUMBERS OF EPISODES, WHEN THE PARAMTERS ARE LEARNED\n","# THE EPISODE WILL BE LONG, AT THAT POINT YOU CAN STOP THE TRAINING PROCESS BY PRESSING CTRL+C\n","# DO NOT WORRY, THE PARAMETERS WILL BE MEMORIZED\n","numberEpisodes=100\n","\n","state_size = 38  # column in dataset exculde label\n","action_size = 2\n","batch_size = 100 # mô hình cập nhật sau khi train 100 dữ liệu\n","# episodes = data.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":525947,"status":"error","timestamp":1685623035969,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"0VSzeP5bvgy0","outputId":"fea0c0e7-1e30-44d2-e32f-fa88cf2d707e"},"outputs":[],"source":["\n","for i in len(dataset):\n","\n","    mini_batch = dataset[i : i + 100]\n","    # create an object\n","    LearningQDeep = DeepQLearning(mini_batch,state_size,action_size,gamma,epsilon,batch_size)\n","    # run the learning process\n","    LearningQDeep.trainingEpisodes()\n","    # get the obtained rewards in every episode\n","    # LearningQDeep.sumRewardsEpisode\n","    #  summarize the model\n","    # LearningQDeep.mainNetwork.summary()\n","    # save the model, this is important, since it takes long time to train the model \n","    # and we will need model in another file to visualize the trained model performance\n","    # LearningQDeep.mainNetwork.save(\"trained_model_temp.h5\")\n","    i += 100\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
