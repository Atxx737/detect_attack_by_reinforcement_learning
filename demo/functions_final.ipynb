{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6750,"status":"ok","timestamp":1685622332901,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"FUs6Oj1Xvgyz"},"outputs":[],"source":["import numpy as np\n","import random\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import RMSprop\n","from collections import deque \n","from tensorflow import gather_nd\n","from tensorflow.keras.losses import mean_squared_error \n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102137,"status":"ok","timestamp":1685622326155,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"8f-bQUBdvgyy","outputId":"072377f3-e2f8-4234-dfa6-a30dc7a986b5"},"outputs":[],"source":["# # Load the Drive helper and mount\n","# from google.colab import drive\n","\n","# # This will prompt for authorization.\n","# drive.mount('/content/drive')\n","\n","# TRAIN_PATH='/content/drive/MyDrive/UIT/HocKy8/KLTN/detect_attack_by_reinforcement_learning/data/matrix1/normal/train-normal.csv'\n","# dataset = pd.read_csv(TRAIN_PATH)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["TRAIN_PATH='../data/matrix1/normal/train-normal.csv'\n","dataset = pd.read_csv(TRAIN_PATH)\n","# print(type(dataset))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":382,"status":"ok","timestamp":1685622507188,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"H_BcnBTMvgyz"},"outputs":[],"source":["class DeepQLearning:\n","    \n","    ###########################################################################\n","    #   START - __init__ function\n","    ###########################################################################\n","    # INPUTS: \n","    # env - Cart Pole environment\n","    # gamma - discount rate\n","    # epsilon - parameter for epsilon-greedy approach\n","    # numberEpisodes - total number of simulation episodes\n","    \n","            \n","    def __init__(self,data,state_size,action_size,gamma,epsilon,batch_size):\n","    \n","        # self.env=env\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.numberEpisodes = 10\n","        self.data = data\n","        # state dimension\n","        # self.stateDimension=4\n","        self.state_size = state_size\n","\n","        # action dimension\n","        # self.actionDimension=2 # right or left\n","        self.action_size = action_size # attack or not\n","\n","        # this is the maximum size of the replay buffer\n","        self.replayBufferSize = 300\n","        # this is the size of the training batch that is randomly sampled from the replay buffer\n","        self.batchReplayBufferSize = batch_size\n","        \n","        # number of training episodes it takes to update the target network parameters\n","        # that is, every updateTargetNetworkPeriod we update the target network parameters\n","        self.updateTargetNetworkPeriod = 100\n","        \n","        # this is the counter for updating the target network \n","        # if this counter exceeds (updateTargetNetworkPeriod-1) we update the network \n","        # parameters and reset the counter to zero, this process is repeated until the end of the training process\n","        self.counterUpdateTargetNetwork = 0\n","        \n","        # this sum is used to store the sum of rewards obtained during each training episode\n","        self.sumRewardsEpisode = []\n","        \n","        # replay buffer\n","        self.replayBuffer = deque(maxlen=self.replayBufferSize)\n","        \n","        # this is the main network\n","        # create network\n","        self.mainNetwork = self.createNetwork()\n","        \n","        # this is the target network\n","        # create network\n","        self.targetNetwork = self.createNetwork()\n","        \n","        # copy the initial weights to targetNetwork\n","        self.targetNetwork.set_weights(self.mainNetwork.get_weights())\n","        \n","        # this list is used in the cost function to select certain entries of the \n","        # predicted and true sample matrices in order to form the loss\n","        self.actionsAppend = []\n","    \n","    ###########################################################################\n","    #   END - __init__ function\n","    ###########################################################################\n","    \n","    ###########################################################################\n","    # START - function for defining the loss (cost) function\n","    # INPUTS: \n","    #\n","    # y_true - matrix of dimension (self.batchReplayBufferSize,2) - this is the target \n","    # y_pred - matrix of dimension (self.batchReplayBufferSize,2) - this is predicted by the network\n","    # \n","    # - this function will select certain row entries from y_true and y_pred to form the output \n","    # the selection is performed on the basis of the action indices in the list  self.actionsAppend\n","    # - this function is used in createNetwork(self) to create the network\n","    #\n","    # OUTPUT: \n","    #    \n","    # - loss - watch out here, this is a vector of (self.batchReplayBufferSize,1), \n","    # with each entry being the squared error between the entries of y_true and y_pred\n","    # later on, the tensor flow will compute the scalar out of this vector (mean squared error)\n","    ###########################################################################    \n","    \n","    def my_loss_fn(self,y_true, y_pred):\n","        \n","        s1,s2=y_true.shape\n","        #print(s1,s2)\n","        \n","        # this matrix defines indices of a set of entries that we want to \n","        # extract from y_true and y_pred\n","        # s2=2\n","        # s1=self.batchReplayBufferSize\n","        indices=np.zeros(shape=(s1,s2))\n","        indices[:,0]=np.arange(s1)\n","        indices[:,1]=self.actionsAppend\n","        \n","        # gather_nd and mean_squared_error are TensorFlow functions\n","        loss = mean_squared_error(gather_nd(y_true,indices=indices.astype(int)), gather_nd(y_pred,indices=indices.astype(int)))\n","        #print(loss)\n","        return loss    \n","    ###########################################################################\n","    #   END - of function my_loss_fn\n","    ###########################################################################\n","    \n","    \n","    ###########################################################################\n","    #   START - function createNetwork()\n","    # this function creates the network\n","    ###########################################################################\n","    \n","    # create a neural network\n","    def  createNetwork(self):\n","        model=Sequential()\n","        # model.add(Dense(128,input_dim=self.stateDimension,activation='relu'))\n","        model.add(Dense(128,input_dim=self.state_size,activation='relu'))\n","        model.add(Dense(56,activation='relu'))\n","        # model.add(Dense(self.actionDimension,activation='linear'))\n","        model.add(Dense(self.action_size,activation='linear'))\n","        # compile the network with the custom loss defined in my_loss_fn\n","        model.compile(optimizer = RMSprop(), loss = self.my_loss_fn, metrics = ['accuracy'])\n","        return model\n","    ###########################################################################\n","    #   END - function createNetwork()\n","    ###########################################################################\n","            \n","    ###########################################################################\n","    #   START - function trainingEpisodes()\n","    #   - this function simulates the episodes and calls the training function \n","    #   - trainNetwork()\n","    ###########################################################################\n","\n","    def trainingEpisodes(self): \n","        \n","        # here we loop through the episodes = 10\n","        for indexEpisode in range(self.numberEpisodes):\n","            \n","            # list that stores rewards per episode - this is necessary for keeping track of convergence \n","            rewardsEpisode=[]\n","                       \n","            print(\"Simulating episode {}\".format(indexEpisode))\n","            \n","            # reset the environment at the beginning of every episode\n","            (currentState,_)=self.reset()\n","            # (currentState,_)=self.env.reset()\n","                      \n","            # here we step from one state to another\n","            # this will loop until a terminal state is reached\n","            # terminalState=False\n","            # while not terminalState:\n","\n","            print(\"len(self.data)\",self.data.shape[0])\n","            print(\"indexEpisode\",indexEpisode)\n","            print(\"len data -1\",self.data.shape[0]-1)                        \n","\n","            # print(\"_____begin_train________\")\n","\n","            for indexSample in range (self.data.shape[0] -1):\n","                # select an action on the basis of the current state, denoted by currentState\n","                action = self.selectAction(currentState,indexSample)\n","                \n","                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n","                # (nextState, reward, terminalState,_,_) = self.env.step(action)       \n","                (nextState, reward, terminalState,_,_) = self.step(action,indexSample)     \n","\n","                rewardsEpisode.append(reward)\n","         \n","                # add current state, action, reward, next state, and terminal flag to the replay buffer\n","                self.replayBuffer.append((currentState,action,reward,nextState,terminalState))\n","                \n","                # train network\n","                self.trainNetwork()\n","                \n","                # set the current state for the next step\n","                currentState=nextState\n","            # print(\"_____end_train________\")\n","            \n","            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n","            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n","    ###########################################################################\n","    #   END - function trainingEpisodes()\n","    ###########################################################################\n","            \n","       \n","    ###########################################################################\n","    #    START - function for selecting an action: epsilon-greedy approach\n","    ###########################################################################\n","    # this function selects an action on the basis of the current state \n","    # INPUTS: \n","    # state - state for which to compute the action\n","    # index - index of the current episode\n","    def selectAction(self,state,index):\n","        import numpy as np\n","        \n","        # first index episodes we select completely random actions to have enough exploration\n","        # change this\n","        if index<1:\n","            return np.random.choice(self.action_size)   \n","            \n","        # Returns a random real number in the half-open interval [0.0, 1.0)\n","        # this number is used for the epsilon greedy approach\n","        randomNumber=np.random.random()\n","\n","        # print(\"round:\",round(self.batchReplayBufferSize * 200 / 1000))\n","\n","        # after index episodes, we slowly start to decrease the epsilon parameter\n","        if index>round(self.batchReplayBufferSize * 200 / 1000):\n","            self.epsilon=0.999*self.epsilon\n","        \n","        # if this condition is satisfied, we are exploring, that is, we select random actions\n","        if randomNumber < self.epsilon:\n","            # returns a random action selected from: 0,1,...,actionNumber-1\n","            return np.random.choice(self.action_size)            \n","        \n","        # otherwise, we are selecting greedy actions\n","        else:\n","            # we return the index where Qvalues[state,:] has the max value\n","            # that is, since the index denotes an action, we select greedy actions\n","                       \n","            # Qvalues=self.mainNetwork.predict(state.reshape(1,4))\n","            Qvalues=self.mainNetwork.predict(state.reshape(1,self.state_size))\n","          \n","            return np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n","            # here we need to return the minimum index since it can happen\n","            # that there are several identical maximal entries, for example \n","            # import numpy as np\n","            # a=[0,1,1,0]\n","            # np.where(a==np.max(a))\n","            # this will return [1,2], but we only need a single index\n","            # that is why we need to have np.random.choice(np.where(a==np.max(a))[0])\n","            # note that zero has to be added here since np.where() returns a tuple\n","    ###########################################################################\n","    #    END - function selecting an action: epsilon-greedy approach\n","    ###########################################################################\n","    \n","    ###########################################################################\n","    #    START - function trainNetwork() - this function trains the network\n","    ###########################################################################\n","    \n","    def trainNetwork(self):\n","\n","        # if the replay buffer has at least batchReplayBufferSize elements,\n","        # then train the model \n","        # otherwise wait until the size of the elements exceeds batchReplayBufferSize\n","        if (len(self.replayBuffer)>self.batchReplayBufferSize):\n","            \n","\n","            # sample a batch from the replay buffer\n","            randomSampleBatch=random.sample(self.replayBuffer, self.batchReplayBufferSize)\n","            \n","            # here we form current state batch \n","            # and next state batch\n","            # they are used as inputs for prediction\n","            # currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))\n","            # nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))    \n","            currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,self.state_size))\n","            nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,self.state_size))            \n","            # this will enumerate the tuple entries of the randomSampleBatch\n","            # index will loop through the number of tuples\n","            for index,tupleS in enumerate(randomSampleBatch):\n","                # first entry of the tuple is the current state\n","                # currentStateBatch[index,:]=tupleS[0]\n","                currentStateBatch[index, :] = np.reshape(tupleS[0], (1, self.state_size))\n","                # fourth entry of the tuple is the next state\n","                # nextStateBatch[index,:]=tupleS[3]\n","                nextStateBatch[index, :] = np.reshape(tupleS[3], (1, self.state_size))\n","            \n","            # here, use the target network to predict Q-values \n","            QnextStateTargetNetwork=self.targetNetwork.predict(nextStateBatch)\n","            # here, use the main network to predict Q-values \n","            QcurrentStateMainNetwork=self.mainNetwork.predict(currentStateBatch)\n","            \n","            # now, we form batches for training\n","            # input for training\n","            inputNetwork=currentStateBatch\n","            # output for training\n","            outputNetwork=np.zeros(shape=(self.batchReplayBufferSize,2))\n","            \n","            # this list will contain the actions that are selected from the batch \n","            # this list is used in my_loss_fn to define the loss-function\n","            self.actionsAppend=[]            \n","            for index,(currentState,action,reward,nextState,terminated) in enumerate(randomSampleBatch):\n","                \n","                # if the next state is the terminal state\n","                if terminated:\n","                    y=reward                  \n","                # if the next state if not the terminal state    \n","                else:\n","                    y=reward+self.gamma*np.max(QnextStateTargetNetwork[index])\n","                \n","                # this is necessary for defining the cost function\n","                self.actionsAppend.append(action)\n","                \n","                # this actually does not matter since we do not use all the entries in the cost function\n","                outputNetwork[index]=QcurrentStateMainNetwork[index]\n","                # this is what matters\n","                outputNetwork[index,action]=y\n","            \n","            # here, we train the network\n","            self.mainNetwork.fit(inputNetwork,outputNetwork,batch_size = self.batchReplayBufferSize, verbose=0,epochs=100)     \n","            \n","            # after updateTargetNetworkPeriod training sessions, update the coefficients \n","            # of the target network\n","            # increase the counter for training the target network\n","            self.counterUpdateTargetNetwork+=1  \n","            if (self.counterUpdateTargetNetwork>(self.updateTargetNetworkPeriod-1)):\n","                # copy the weights to targetNetwork\n","                self.targetNetwork.set_weights(self.mainNetwork.get_weights())        \n","                print(\"Target network updated!\")\n","                print(\"Counter value {}\".format(self.counterUpdateTargetNetwork))\n","                # reset the counter\n","                self.counterUpdateTargetNetwork=0\n","    ###########################################################################\n","    #    END - function trainNetwork() \n","    ###########################################################################     \n","\n","    ###########################################################################\n","    # START - function for defining the reset function for ENV \n","    # reset the environment at the beginning of every episode\n","    ###########################################################################                \n","            \n","    def step(self,action,index):\n","        \n","        # super().reset(seed=seed)\n","        # Note that if you use custom reset bounds, it may lead to out-of-bound\n","        # state/observations.\n","        # print(\"*\",index ,\"*\")\n","        # print(\"action\",action)\n","        # print(\"label\",self.data.iloc[index].values[-1])\n","        if action == self.data.iloc[index].values[-1]: \n","            reward = 1\n","        else:  \n","            reward = -1\n","        # print (\"reward\",reward)\n","        next_state = self.data.iloc[index + 1].values[:-1]\n","        next_state = np.reshape(next_state, [1, self.state_size])\n","\n","        # print(self.data.iloc[index].values[:-1])\n","        # print(\"nextState\")\n","        # print(self.data.iloc[index + 1].values[:-1])\n","        \n","        if index == (self.data.shape[0]-1):\n","            terminated = True\n","        terminated = False\n","        self.state = next_state\n","        # print(\"_____________\")\n","        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n","\n","    ###########################################################################\n","    #    END - function for defining the reset function for ENV \n","    ###########################################################################                \n","                \n","    ###########################################################################\n","    # START - function for defining the reset function for ENV \n","    # reset the environment at the beginning of every episode\n","    ###########################################################################                \n","            \n","    def reset(self):\n","      \n","        # super().reset(seed=seed)\n","        # Note that if you use custom reset bounds, it may lead to out-of-bound\n","        # state/observations.\n","        # self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n","\n","        #random index in mini-batch\n","        start_index = np.random.randint(0, len(self.data) - 1)\n","\n","        self.steps_beyond_terminated = None\n","        self.state =  self.data.iloc[start_index].values[:-1]\n","        # self.label =  self.data.iloc[start_index].values[-1]\n","\n","        return np.array(self.state, dtype=np.float32), {}\n","\n","    ###########################################################################\n","    #    END - function for defining the reset function for ENV \n","    ###########################################################################        \n","            "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685622336822,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"rzSiskhpvgy0"},"outputs":[],"source":["# select the parameters\n","gamma=1\n","# probability parameter for the epsilon-greedy approach\n","epsilon=0.1\n","# number of training episodes\n","# NOTE HERE THAT AFTER CERTAIN NUMBERS OF EPISODES, WHEN THE PARAMTERS ARE LEARNED\n","# THE EPISODE WILL BE LONG, AT THAT POINT YOU CAN STOP THE TRAINING PROCESS BY PRESSING CTRL+C\n","# DO NOT WORRY, THE PARAMETERS WILL BE MEMORIZED\n","# numberEpisodes=100\n","\n","state_size = 38  # column in dataset exculde label\n","action_size = 2\n","batch_size = 100 # mô hình cập nhật sau khi train 100 dữ liệu\n","# episodes = data.shape[0]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":525947,"status":"error","timestamp":1685623035969,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"0VSzeP5bvgy0","outputId":"fea0c0e7-1e30-44d2-e32f-fa88cf2d707e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Simulating episode 0\n","len(self.data) 100\n","indexEpisode 0\n","len data -1 99\n","_____begin_train________\n","* 0 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 3 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 1 2 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 2 0 4 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 196ms/step\n","* 1 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 1 2 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 2 0 4 0 0 2 0 0 0\n"," 0]\n","nextState\n","[ 0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n"," 32  0  0  0  0 16  2  7  6  4  0  0  0  0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 52ms/step\n","* 2 *\n","action 1\n","label 1\n","reward 1\n","[ 0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n"," 32  0  0  0  0 16  2  7  6  4  0  0  0  0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 56ms/step\n","* 3 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 39ms/step\n","* 4 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[7 0 0 0 0 0 0 2 0 0 0 2 0 2 0 0 0 0 0 0 0 0 8 0 1 0 0 0 0 0 4 0 1 3 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 42ms/step\n","* 5 *\n","action 1\n","label 1\n","reward 1\n","[7 0 0 0 0 0 0 2 0 0 0 2 0 2 0 0 0 0 0 0 0 0 8 0 1 0 0 0 0 0 4 0 1 3 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 0 1 1 0 0 2 6 0 0 0 0 0 0 0 0 5 0 0 0 2 1 3 1 2 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 40ms/step\n","* 6 *\n","action 0\n","label 1\n","reward -1\n","[0 0 0 0 0 0 0 2 0 0 1 1 0 0 2 6 0 0 0 0 0 0 0 0 5 0 0 0 2 1 3 1 2 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 35ms/step\n","* 7 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 1 4 0 0 0 1 6 2 1 2 0 0 0 0 0 0 0 2 4 6 0 0 0 5 2 0 2 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","* 8 *\n","action 0\n","label 1\n","reward -1\n","[0 0 0 0 0 0 1 4 0 0 0 1 6 2 1 2 0 0 0 0 0 0 0 2 4 6 0 0 0 5 2 0 2 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 43ms/step\n","* 9 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 1 0 3 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 1 0 7 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 44ms/step\n","* 10 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 1 0 3 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 1 0 7 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 47ms/step\n","* 11 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 41ms/step\n","* 12 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 43ms/step\n","* 13 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 40ms/step\n","* 14 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 39ms/step\n","* 15 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[ 0  0  0  0  0  0  0  0  0 12  0 13  0  0  0  0  0  0  0  0  0  0  3  0\n"," 11  0  0  0  0  4 29  4  3  1  0  0  0  0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 39ms/step\n","* 16 *\n","action 1\n","label 1\n","reward 1\n","[ 0  0  0  0  0  0  0  0  0 12  0 13  0  0  0  0  0  0  0  0  0  0  3  0\n"," 11  0  0  0  0  4 29  4  3  1  0  0  0  0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","* 17 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 43ms/step\n","* 18 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 39ms/step\n","* 19 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 36ms/step\n","* 20 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 2 0 2 6 0 1 0 0 0 0 0 0 0 4 0 1 0 0 0 1 6 7 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 40ms/step\n","* 21 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 2 0 2 6 0 1 0 0 0 0 0 0 0 4 0 1 0 0 0 1 6 7 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","* 22 *\n","action 0\n","label 0\n","reward 1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 38ms/step\n","* 23 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 38ms/step\n","* 24 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 36ms/step\n","* 25 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 41ms/step\n","* 26 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 37ms/step\n","* 27 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","* 28 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[ 0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n"," 38  0  0  0  0 11  2 14 12  2  0  0  0  0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 35ms/step\n","* 29 *\n","action 1\n","label 1\n","reward 1\n","[ 0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n"," 38  0  0  0  0 11  2 14 12  2  0  0  0  0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","* 30 *\n","action 0\n","label 0\n","reward 1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","nextState\n","[ 0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n"," 32  0  0  0  0 14  2  9  6  4  0  0  0  0]\n","_____________\n","round: 20\n","* 31 *\n","action 1\n","label 1\n","reward 1\n","[ 0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0\n"," 32  0  0  0  0 14  2  9  6  4  0  0  0  0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 37ms/step\n","* 32 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 37ms/step\n","* 33 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","nextState\n","[2 0 0 0 0 0 0 2 0 0 0 1 0 4 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 3 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 38ms/step\n","* 34 *\n","action 1\n","label 1\n","reward 1\n","[2 0 0 0 0 0 0 2 0 0 0 1 0 4 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 3 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 2 0 2 1 0 3 0 0 0 0 0 1 0 1 0 1 0 0 0 4 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 48ms/step\n","* 35 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 0 2 0 2 1 0 3 0 0 0 0 0 1 0 1 0 1 0 0 0 4 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 2 0 2 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 4 0 1 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 39ms/step\n","* 36 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 0 2 0 2 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 4 0 1 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 1 0 1 0 0 1 6 0 0 0 0 0 0 0 0 6 0 0 0 1 1 5 0 2 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 36ms/step\n","* 37 *\n","action 0\n","label 1\n","reward -1\n","[0 0 0 0 0 0 0 2 0 1 0 1 0 0 1 6 0 0 0 0 0 0 0 0 6 0 0 0 1 1 5 0 2 1 0 0 0\n"," 0]\n","nextState\n","[ 0  0  0  0  0  0  0  0  0  0  0  0  0 24  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0 23  2  0  0  0  0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 42ms/step\n","* 38 *\n","action 1\n","label 0\n","reward -1\n","[ 0  0  0  0  0  0  0  0  0  0  0  0  0 24  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  0  0  0  0 23  2  0  0  0  0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 5 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 35ms/step\n","* 39 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 5 0 0 2 0 0 0\n"," 0]\n","nextState\n","[ 0  0  0  0  0  0  0  0  0 12  0 13  0  0  0  0  0  0  0  0  0  0  3  0\n"," 13  0  0  0  1  3 29  1  7  1  0  0  0  0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 36ms/step\n","* 40 *\n","action 1\n","label 1\n","reward 1\n","[ 0  0  0  0  0  0  0  0  0 12  0 13  0  0  0  0  0  0  0  0  0  0  3  0\n"," 13  0  0  0  1  3 29  1  7  1  0  0  0  0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 37ms/step\n","* 41 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 37ms/step\n","* 42 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 39ms/step\n","* 43 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 41ms/step\n","* 44 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 4 0 0 3 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 42ms/step\n","* 45 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 4 0 0 3 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 5 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 49ms/step\n","* 46 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 5 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 56ms/step\n","* 47 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","* 48 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 38ms/step\n","* 49 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 53ms/step\n","* 50 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 45ms/step\n","* 51 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 46ms/step\n","* 52 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 2 6 0 0 0 0 0 0 0 0 8 0 0 0 1 1 3 3 3 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 46ms/step\n","* 53 *\n","action 0\n","label 1\n","reward -1\n","[0 0 0 0 0 0 0 1 0 0 0 1 0 0 2 6 0 0 0 0 0 0 0 0 8 0 0 0 1 1 3 3 3 1 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 1 0 0 0 2 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 5 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 45ms/step\n","* 54 *\n","action 1\n","label 1\n","reward 1\n","[0 0 0 0 0 0 0 1 0 0 0 2 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 5 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n"," 0]\n","_____________\n","round: 20\n","1/1 [==============================] - 0s 41ms/step\n","* 55 *\n","action 1\n","label 0\n","reward -1\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n"," 0]\n","nextState\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0]\n","_____________\n","round: 20\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\fujos\\KLTN\\detect_attack_by_reinforcement_learning\\demo\\functions_final.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m LearningQDeep \u001b[39m=\u001b[39m DeepQLearning(mini_batch,state_size,action_size,gamma,epsilon,batch_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# run the learning process\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m LearningQDeep\u001b[39m.\u001b[39;49mtrainingEpisodes()\n","\u001b[1;32mc:\\Users\\fujos\\KLTN\\detect_attack_by_reinforcement_learning\\demo\\functions_final.ipynb Cell 6\u001b[0m in \u001b[0;36mDeepQLearning.trainingEpisodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m_____begin_train________\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39mfor\u001b[39;00m indexSample \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     \u001b[39m# select an action on the basis of the current state, denoted by currentState\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselectAction(currentState,indexSample)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m     \u001b[39m# here we step and return the state, reward, and boolean denoting if the state is a terminal state\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     \u001b[39m# (nextState, reward, terminalState,_,_) = self.env.step(action)       \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m     (nextState, reward, terminalState,_,_) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(action,indexSample)     \n","\u001b[1;32mc:\\Users\\fujos\\KLTN\\detect_attack_by_reinforcement_learning\\demo\\functions_final.ipynb Cell 6\u001b[0m in \u001b[0;36mDeepQLearning.selectAction\u001b[1;34m(self, state, index)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_size)            \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m \u001b[39m# otherwise, we are selecting greedy actions\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m     \u001b[39m# we return the index where Qvalues[state,:] has the max value\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m     \u001b[39m# that is, since the index denotes an action, we select greedy actions\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m                \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m     \u001b[39m# Qvalues=self.mainNetwork.predict(state.reshape(1,4))\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m     Qvalues\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmainNetwork\u001b[39m.\u001b[39;49mpredict(state\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate_size))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fujos/KLTN/detect_attack_by_reinforcement_learning/demo/functions_final.ipynb#W5sZmlsZQ%3D%3D?line=221'>222</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39mwhere(Qvalues[\u001b[39m0\u001b[39m,:]\u001b[39m==\u001b[39mnp\u001b[39m.\u001b[39mmax(Qvalues[\u001b[39m0\u001b[39m,:]))[\u001b[39m0\u001b[39m])\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:2378\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2376\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2377\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2378\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2379\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2380\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1305\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1303\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1305\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1306\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1307\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    504\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    506\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:713\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    709\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    711\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    712\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 713\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:752\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    749\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[0;32m    750\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[0;32m    751\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 752\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3439\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3437\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3438\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3439\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3440\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3441\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3442\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(0,len(dataset),batch_size):\n","\n","    mini_batch = dataset[i : i + batch_size]\n","    # print(mini_batch)\n","    # create an object\n","    LearningQDeep = DeepQLearning(mini_batch,state_size,action_size,gamma,epsilon,batch_size)\n","    # run the learning process\n","    LearningQDeep.trainingEpisodes()\n","    # get the obtained rewards in every episode\n","    # LearningQDeep.sumRewardsEpisode\n","    #  summarize the model\n","    # LearningQDeep.mainNetwork.summary()\n","    # save the model, this is important, since it takes long time to train the model \n","    # and we will need model in another file to visualize the trained model performance\n","    # LearningQDeep.mainNetwork.save(\"trained_model_temp.h5\")\n","\n","    print(\"done\",i,\"in dataset \",len(dataset))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mini_batch = dataset[0 : 0 + batch_size]\n","print(mini_batch)\n","# print(mini_batch.shape[0])\n","# for indexSample in range (len(mini_batch)-1):\n","#     print(\"*\",indexSample ,\"*\")\n","#     print(mini_batch.iloc[indexSample])\n","#     print(\"nextState\")\n","#     print(mini_batch.iloc[indexSample+1])\n","print(\"-------\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
