{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from collections import deque \n",
    "from tensorflow import gather_nd\n",
    "from tensorflow.keras.losses import mean_squared_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning:\n",
    "    \n",
    "    ###########################################################################\n",
    "    #   START - __init__ function\n",
    "    ###########################################################################\n",
    "    # INPUTS: \n",
    "    # env - Cart Pole environment\n",
    "    # gamma - discount rate\n",
    "    # epsilon - parameter for epsilon-greedy approach\n",
    "    # numberEpisodes - total number of simulation episodes\n",
    "    \n",
    "            \n",
    "    def __init__(self,state_size,action_size,gamma,epsilon,numberEpisodes):\n",
    "    \n",
    "        \n",
    "        # self.env=env\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.numberEpisodes=numberEpisodes\n",
    "        \n",
    "        # state dimension\n",
    "        # self.stateDimension=4\n",
    "        self.state_size=state_size\n",
    "\n",
    "        # action dimension\n",
    "        # self.actionDimension=2 # right or left\n",
    "        self.action_size=action_size # right or left\n",
    "\n",
    "        # this is the maximum size of the replay buffer\n",
    "        self.replayBufferSize=300\n",
    "        # this is the size of the training batch that is randomly sampled from the replay buffer\n",
    "        self.batchReplayBufferSize=100\n",
    "        \n",
    "        # number of training episodes it takes to update the target network parameters\n",
    "        # that is, every updateTargetNetworkPeriod we update the target network parameters\n",
    "        self.updateTargetNetworkPeriod=100\n",
    "        \n",
    "        # this is the counter for updating the target network \n",
    "        # if this counter exceeds (updateTargetNetworkPeriod-1) we update the network \n",
    "        # parameters and reset the counter to zero, this process is repeated until the end of the training process\n",
    "        self.counterUpdateTargetNetwork=0\n",
    "        \n",
    "        # this sum is used to store the sum of rewards obtained during each training episode\n",
    "        self.sumRewardsEpisode=[]\n",
    "        \n",
    "        # replay buffer\n",
    "        self.replayBuffer=deque(maxlen=self.replayBufferSize)\n",
    "        \n",
    "        # this is the main network\n",
    "        # create network\n",
    "        self.mainNetwork=self.createNetwork()\n",
    "        \n",
    "        # this is the target network\n",
    "        # create network\n",
    "        self.targetNetwork=self.createNetwork()\n",
    "        \n",
    "        # copy the initial weights to targetNetwork\n",
    "        self.targetNetwork.set_weights(self.mainNetwork.get_weights())\n",
    "        \n",
    "        # this list is used in the cost function to select certain entries of the \n",
    "        # predicted and true sample matrices in order to form the loss\n",
    "        self.actionsAppend=[]\n",
    "    \n",
    "    ###########################################################################\n",
    "    #   END - __init__ function\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    # START - function for defining the loss (cost) function\n",
    "    # INPUTS: \n",
    "    #\n",
    "    # y_true - matrix of dimension (self.batchReplayBufferSize,2) - this is the target \n",
    "    # y_pred - matrix of dimension (self.batchReplayBufferSize,2) - this is predicted by the network\n",
    "    # \n",
    "    # - this function will select certain row entries from y_true and y_pred to form the output \n",
    "    # the selection is performed on the basis of the action indices in the list  self.actionsAppend\n",
    "    # - this function is used in createNetwork(self) to create the network\n",
    "    #\n",
    "    # OUTPUT: \n",
    "    #    \n",
    "    # - loss - watch out here, this is a vector of (self.batchReplayBufferSize,1), \n",
    "    # with each entry being the squared error between the entries of y_true and y_pred\n",
    "    # later on, the tensor flow will compute the scalar out of this vector (mean squared error)\n",
    "    ###########################################################################    \n",
    "    \n",
    "    def my_loss_fn(self,y_true, y_pred):\n",
    "        \n",
    "        s1,s2=y_true.shape\n",
    "        #print(s1,s2)\n",
    "        \n",
    "        # this matrix defines indices of a set of entries that we want to \n",
    "        # extract from y_true and y_pred\n",
    "        # s2=2\n",
    "        # s1=self.batchReplayBufferSize\n",
    "        indices=np.zeros(shape=(s1,s2))\n",
    "        indices[:,0]=np.arange(s1)\n",
    "        indices[:,1]=self.actionsAppend\n",
    "        \n",
    "        # gather_nd and mean_squared_error are TensorFlow functions\n",
    "        loss = mean_squared_error(gather_nd(y_true,indices=indices.astype(int)), gather_nd(y_pred,indices=indices.astype(int)))\n",
    "        #print(loss)\n",
    "        return loss    \n",
    "    ###########################################################################\n",
    "    #   END - of function my_loss_fn\n",
    "    ###########################################################################\n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    #   START - function createNetwork()\n",
    "    # this function creates the network\n",
    "    ###########################################################################\n",
    "    \n",
    "    # create a neural network\n",
    "    def  createNetwork(self):\n",
    "        model=Sequential()\n",
    "        # model.add(Dense(128,input_dim=self.stateDimension,activation='relu'))\n",
    "        model.add(Dense(128,input_dim=self.state_size,activation='relu'))\n",
    "        model.add(Dense(56,activation='relu'))\n",
    "        # model.add(Dense(self.actionDimension,activation='linear'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        # compile the network with the custom loss defined in my_loss_fn\n",
    "        model.compile(optimizer = RMSprop(), loss = self.my_loss_fn, metrics = ['accuracy'])\n",
    "        return model\n",
    "    ###########################################################################\n",
    "    #   END - function createNetwork()\n",
    "    ###########################################################################\n",
    "            \n",
    "    ###########################################################################\n",
    "    #   START - function trainingEpisodes()\n",
    "    #   - this function simulates the episodes and calls the training function \n",
    "    #   - trainNetwork()\n",
    "    ###########################################################################\n",
    "\n",
    "    def trainingEpisodes(self):\n",
    "   \n",
    "        \n",
    "        # here we loop through the episodes\n",
    "        for indexEpisode in range(self.numberEpisodes):\n",
    "            \n",
    "            # list that stores rewards per episode - this is necessary for keeping track of convergence \n",
    "            rewardsEpisode=[]\n",
    "                       \n",
    "            print(\"Simulating episode {}\".format(indexEpisode))\n",
    "            \n",
    "            # reset the environment at the beginning of every episode\n",
    "            (currentState,_)=self.reset()\n",
    "            # (currentState,_)=self.env.reset()\n",
    "                      \n",
    "            # here we step from one state to another\n",
    "            # this will loop until a terminal state is reached\n",
    "            terminalState=False\n",
    "            while not terminalState:\n",
    "                                      \n",
    "                # select an action on the basis of the current state, denoted by currentState\n",
    "                action = self.selectAction(currentState,indexEpisode)\n",
    "                \n",
    "                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "                # (nextState, reward, terminalState,_,_) = self.env.step(action)       \n",
    "                (nextState, reward, terminalState,_,_) = self.step(action)       \n",
    "                   \n",
    "                rewardsEpisode.append(reward)\n",
    "         \n",
    "                # add current state, action, reward, next state, and terminal flag to the replay buffer\n",
    "                self.replayBuffer.append((currentState,action,reward,nextState,terminalState))\n",
    "                \n",
    "                # train network\n",
    "                self.trainNetwork()\n",
    "                \n",
    "                # set the current state for the next step\n",
    "                currentState=nextState\n",
    "            \n",
    "            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n",
    "            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n",
    "    ###########################################################################\n",
    "    #   END - function trainingEpisodes()\n",
    "    ###########################################################################\n",
    "            \n",
    "       \n",
    "    ###########################################################################\n",
    "    #    START - function for selecting an action: epsilon-greedy approach\n",
    "    ###########################################################################\n",
    "    # this function selects an action on the basis of the current state \n",
    "    # INPUTS: \n",
    "    # state - state for which to compute the action\n",
    "    # index - index of the current episode\n",
    "    def selectAction(self,state,index):\n",
    "        import numpy as np\n",
    "        \n",
    "        # first index episodes we select completely random actions to have enough exploration\n",
    "        # change this\n",
    "        if index<1:\n",
    "            return np.random.choice(self.actionDimension)   \n",
    "            \n",
    "        # Returns a random real number in the half-open interval [0.0, 1.0)\n",
    "        # this number is used for the epsilon greedy approach\n",
    "        randomNumber=np.random.random()\n",
    "        \n",
    "        # after index episodes, we slowly start to decrease the epsilon parameter\n",
    "        if index>200:\n",
    "            self.epsilon=0.999*self.epsilon\n",
    "        \n",
    "        # if this condition is satisfied, we are exploring, that is, we select random actions\n",
    "        if randomNumber < self.epsilon:\n",
    "            # returns a random action selected from: 0,1,...,actionNumber-1\n",
    "            return np.random.choice(self.actionDimension)            \n",
    "        \n",
    "        # otherwise, we are selecting greedy actions\n",
    "        else:\n",
    "            # we return the index where Qvalues[state,:] has the max value\n",
    "            # that is, since the index denotes an action, we select greedy actions\n",
    "                       \n",
    "            Qvalues=self.mainNetwork.predict(state.reshape(1,4))\n",
    "          \n",
    "            return np.random.choice(np.where(Qvalues[0,:]==np.max(Qvalues[0,:]))[0])\n",
    "            # here we need to return the minimum index since it can happen\n",
    "            # that there are several identical maximal entries, for example \n",
    "            # import numpy as np\n",
    "            # a=[0,1,1,0]\n",
    "            # np.where(a==np.max(a))\n",
    "            # this will return [1,2], but we only need a single index\n",
    "            # that is why we need to have np.random.choice(np.where(a==np.max(a))[0])\n",
    "            # note that zero has to be added here since np.where() returns a tuple\n",
    "    ###########################################################################\n",
    "    #    END - function selecting an action: epsilon-greedy approach\n",
    "    ###########################################################################\n",
    "    \n",
    "    ###########################################################################\n",
    "    #    START - function trainNetwork() - this function trains the network\n",
    "    ###########################################################################\n",
    "    \n",
    "    def trainNetwork(self):\n",
    "\n",
    "        # if the replay buffer has at least batchReplayBufferSize elements,\n",
    "        # then train the model \n",
    "        # otherwise wait until the size of the elements exceeds batchReplayBufferSize\n",
    "        if (len(self.replayBuffer)>self.batchReplayBufferSize):\n",
    "            \n",
    "\n",
    "            # sample a batch from the replay buffer\n",
    "            randomSampleBatch=random.sample(self.replayBuffer, self.batchReplayBufferSize)\n",
    "            \n",
    "            # here we form current state batch \n",
    "            # and next state batch\n",
    "            # they are used as inputs for prediction\n",
    "            currentStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))\n",
    "            nextStateBatch=np.zeros(shape=(self.batchReplayBufferSize,4))            \n",
    "            # this will enumerate the tuple entries of the randomSampleBatch\n",
    "            # index will loop through the number of tuples\n",
    "            for index,tupleS in enumerate(randomSampleBatch):\n",
    "                # first entry of the tuple is the current state\n",
    "                currentStateBatch[index,:]=tupleS[0]\n",
    "                # fourth entry of the tuple is the next state\n",
    "                nextStateBatch[index,:]=tupleS[3]\n",
    "            \n",
    "            # here, use the target network to predict Q-values \n",
    "            QnextStateTargetNetwork=self.targetNetwork.predict(nextStateBatch)\n",
    "            # here, use the main network to predict Q-values \n",
    "            QcurrentStateMainNetwork=self.mainNetwork.predict(currentStateBatch)\n",
    "            \n",
    "            # now, we form batches for training\n",
    "            # input for training\n",
    "            inputNetwork=currentStateBatch\n",
    "            # output for training\n",
    "            outputNetwork=np.zeros(shape=(self.batchReplayBufferSize,2))\n",
    "            \n",
    "            # this list will contain the actions that are selected from the batch \n",
    "            # this list is used in my_loss_fn to define the loss-function\n",
    "            self.actionsAppend=[]            \n",
    "            for index,(currentState,action,reward,nextState,terminated) in enumerate(randomSampleBatch):\n",
    "                \n",
    "                # if the next state is the terminal state\n",
    "                if terminated:\n",
    "                    y=reward                  \n",
    "                # if the next state if not the terminal state    \n",
    "                else:\n",
    "                    y=reward+self.gamma*np.max(QnextStateTargetNetwork[index])\n",
    "                \n",
    "                # this is necessary for defining the cost function\n",
    "                self.actionsAppend.append(action)\n",
    "                \n",
    "                # this actually does not matter since we do not use all the entries in the cost function\n",
    "                outputNetwork[index]=QcurrentStateMainNetwork[index]\n",
    "                # this is what matters\n",
    "                outputNetwork[index,action]=y\n",
    "            \n",
    "            # here, we train the network\n",
    "            self.mainNetwork.fit(inputNetwork,outputNetwork,batch_size = self.batchReplayBufferSize, verbose=0,epochs=100)     \n",
    "            \n",
    "            # after updateTargetNetworkPeriod training sessions, update the coefficients \n",
    "            # of the target network\n",
    "            # increase the counter for training the target network\n",
    "            self.counterUpdateTargetNetwork+=1  \n",
    "            if (self.counterUpdateTargetNetwork>(self.updateTargetNetworkPeriod-1)):\n",
    "                # copy the weights to targetNetwork\n",
    "                self.targetNetwork.set_weights(self.mainNetwork.get_weights())        \n",
    "                print(\"Target network updated!\")\n",
    "                print(\"Counter value {}\".format(self.counterUpdateTargetNetwork))\n",
    "                # reset the counter\n",
    "                self.counterUpdateTargetNetwork=0\n",
    "    ###########################################################################\n",
    "    #    END - function trainNetwork() \n",
    "    ###########################################################################     \n",
    "                  \n",
    "                \n",
    "    ###########################################################################\n",
    "    # START - function for defining the reset function for ENV \n",
    "    # reset the environment at the beginning of every episode\n",
    "    ###########################################################################                \n",
    "            \n",
    "    def reset(self):\n",
    "      \n",
    "        # super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        # self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        string = \"0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,3,0,0,1,0,0,0,0\"\n",
    "        array = np.array([float(x) for x in string.split(\",\")])\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(array, dtype=np.float32), {}\n",
    "\n",
    "    ###########################################################################\n",
    "    #    END - function for defining the reset function for ENV \n",
    "    ###########################################################################        \n",
    "            \n",
    "\n",
    "###########################################################################\n",
    "# START - function for defining the reset function for ENV \n",
    "# reset the environment at the beginning of every episode\n",
    "###########################################################################                \n",
    "        \n",
    "def step(self, action):\n",
    "    \n",
    "    # super().reset(seed=seed)\n",
    "    # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "    # state/observations.\n",
    "    # self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n",
    "    self.steps_beyond_terminated = None\n",
    "\n",
    "    string = \"0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,3,0,0,1,0,0,0,0\"\n",
    "    array = np.array([float(x) for x in string.split(\",\")])\n",
    "    if self.render_mode == \"human\":\n",
    "        self.render()\n",
    "    return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "###########################################################################\n",
    "#    END - function for defining the reset function for ENV \n",
    "###########################################################################            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
