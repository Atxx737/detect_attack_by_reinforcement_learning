{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6750,"status":"ok","timestamp":1685622332901,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"FUs6Oj1Xvgyz"},"outputs":[],"source":["import numpy as np\n","import random\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import RMSprop\n","from collections import deque \n","from tensorflow import gather_nd\n","from tensorflow.keras.losses import mean_squared_error \n","import pandas as pd\n","from functions_final import DeepQLearning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102137,"status":"ok","timestamp":1685622326155,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"8f-bQUBdvgyy","outputId":"072377f3-e2f8-4234-dfa6-a30dc7a986b5"},"outputs":[],"source":["# # Load the Drive helper and mount\n","# from google.colab import drive\n","\n","# # This will prompt for authorization.\n","# drive.mount('/content/drive')\n","\n","# TRAIN_PATH='/content/drive/MyDrive/UIT/HocKy8/KLTN/detect_attack_by_reinforcement_learning/data/matrix1/normal/train-normal.csv'\n","# dataset = pd.read_csv(TRAIN_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TRAIN_PATH='../data/matrix1/normal/train-normal.csv'\n","dataset = pd.read_csv(TRAIN_PATH)\n","# print(type(dataset))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685622336822,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"rzSiskhpvgy0"},"outputs":[],"source":["# select the parameters\n","gamma=1\n","# probability parameter for the epsilon-greedy approach\n","epsilon=0.1\n","# number of training episodes\n","# NOTE HERE THAT AFTER CERTAIN NUMBERS OF EPISODES, WHEN THE PARAMTERS ARE LEARNED\n","# THE EPISODE WILL BE LONG, AT THAT POINT YOU CAN STOP THE TRAINING PROCESS BY PRESSING CTRL+C\n","# DO NOT WORRY, THE PARAMETERS WILL BE MEMORIZED\n","# numberEpisodes=100\n","\n","state_size = 38  # column in dataset exculde label\n","action_size = 2\n","batch_size = 100 # mô hình cập nhật sau khi train 100 dữ liệu\n","# episodes = data.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":525947,"status":"error","timestamp":1685623035969,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"},"user_tz":-420},"id":"0VSzeP5bvgy0","outputId":"fea0c0e7-1e30-44d2-e32f-fa88cf2d707e"},"outputs":[],"source":["for i in range(0,len(dataset),batch_size):\n","\n","    mini_batch = dataset[i : i + batch_size]\n","    # print(mini_batch)\n","    # create an object\n","    LearningQDeep = DeepQLearning(mini_batch,state_size,action_size,gamma,epsilon,batch_size)\n","    # run the learning process\n","    LearningQDeep.trainingEpisodes()\n","    # get the obtained rewards in every episode\n","    LearningQDeep.sumRewardsEpisode\n","    #  summarize the model\n","    LearningQDeep.mainNetwork.summary()\n","    # save the model, this is important, since it takes long time to train the model \n","    # and we will need model in another file to visualize the trained model performance\n","    LearningQDeep.mainNetwork.save(\"trained_model_temp.h5\")\n","\n","    print(\"done\",i,\"in dataset \",len(dataset))\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
