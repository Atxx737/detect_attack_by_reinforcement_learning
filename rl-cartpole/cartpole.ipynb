{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"id":"ga_T5LaWwinH","executionInfo":{"status":"ok","timestamp":1682443605634,"user_tz":-420,"elapsed":1,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"outputs":[],"source":["# # Load the Drive helper and mount\n","# from google.colab import drive\n","\n","# # This will prompt for authorization.\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yZVIPjWY2x3R"},"source":["## install requiment"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8fnX0x0xVkt","executionInfo":{"status":"ok","timestamp":1682443624006,"user_tz":-420,"elapsed":15052,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"a2013d3f-1211-4602-83d4-6eb0ff85f5a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (3.8.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py) (1.22.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.4.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.9/dist-packages (1.0.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (67.7.2)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.12.2)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.20.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.32.0)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.12.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.53.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (16.0.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2) (0.1.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2) (1.10.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.27.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.0.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.2.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.7.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (6.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.2.2)\n"]}],"source":["!pip install h5py\n","!pip install gym\n","!pip install keras-rl2"]},{"cell_type":"markdown","metadata":{"id":"f-OKkV8l20zM"},"source":["## import module"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"O3B6SKKzwNx5","executionInfo":{"status":"ok","timestamp":1682443624007,"user_tz":-420,"elapsed":20,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"outputs":[],"source":["import numpy as np\n","import gym\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten, InputLayer\n","from keras.optimizers import Adam\n","\n","from rl.agents.dqn import DQNAgent\n","from rl.policy import EpsGreedyQPolicy\n","from rl.memory import SequentialMemory"]},{"cell_type":"markdown","metadata":{"id":"IU_Z8z3n24Nu"},"source":["## create env"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3afmCn8Zwk4b","executionInfo":{"status":"ok","timestamp":1682443624008,"user_tz":-420,"elapsed":20,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"c4bcae04-fb09-47a5-8e4c-b8383d8b0afb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}],"source":["ENV_NAME = 'CartPole-v1'\n","new_step_api = True\n","env = gym.make(ENV_NAME)\n","np.random.seed(123)\n","env.seed(seed=123)\n","# env.reset(seed=123)\n","nb_actions = env.action_space.n #Trong game này nb_actions = 2 ứng với sang trái/phải"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10-6snc7AoQl","executionInfo":{"status":"ok","timestamp":1682443624008,"user_tz":-420,"elapsed":17,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"98bc7584-7d9c-451f-e710-2f765abe2393"},"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}],"source":["print(np.random.seed(123))"]},{"cell_type":"markdown","metadata":{"id":"iWPoYSqfGk2L"},"source":["set up necessary hyperparameters with sensible defaults"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"0zg-nsVSGh-i","executionInfo":{"status":"ok","timestamp":1682443624008,"user_tz":-420,"elapsed":15,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"outputs":[],"source":["discount_factor = 0.95\n","eps = 0.5\n","eps_decay_factor = 0.999\n","num_episodes=500"]},{"cell_type":"markdown","metadata":{"id":"KdyNNaFS25vO"},"source":["## build Neural Network"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bMg1VbgnGsnR","executionInfo":{"status":"ok","timestamp":1682443624008,"user_tz":-420,"elapsed":14,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"outputs":[],"source":["model = Sequential()\n","obs_space_shape = env.observation_space.shape\n","# obs_space_shape = env.observation_space.n\n","model.add(InputLayer(batch_input_shape=(1, obs_space_shape[0])))\n","model.add(Dense(20, activation='relu'))\n","model.add(Dense(env.action_space.n, activation='linear'))\n","model.compile(loss='mse', optimizer='adam', metrics=['mae'])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lGp-2CCuwnW4","executionInfo":{"status":"ok","timestamp":1682443624009,"user_tz":-420,"elapsed":15,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"a9f782c5-744e-4e7d-b3fd-e7039d3d75b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 40)                0         \n","                                                                 \n"," dense_4 (Dense)             (None, 32)                1312      \n","                                                                 \n"," activation (Activation)     (None, 32)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 16)                528       \n","                                                                 \n"," activation_1 (Activation)   (None, 16)                0         \n","                                                                 \n"," dense_6 (Dense)             (None, 2)                 34        \n","                                                                 \n"," activation_2 (Activation)   (None, 2)                 0         \n","                                                                 \n","=================================================================\n","Total params: 1,874\n","Trainable params: 1,874\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["model = Sequential()\n","model.add(Flatten(input_shape=(10,) + env.observation_space.shape))\n","model.add(Dense(32))\n","model.add(Activation('relu'))\n","model.add(Dense(16))\n","model.add(Activation('relu'))\n","model.add(Dense(nb_actions))\n","model.add(Activation('linear'))\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"hUKO415h2_kz"},"source":["## Agent, Policy và training"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"OaKxv5wvwqMu","executionInfo":{"status":"error","timestamp":1682445004112,"user_tz":-420,"elapsed":190613,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"0cd2dcdc-a891-4208-cb73-b425bf7a4882"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training for 20000 steps ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 10 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 11 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 12 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 13 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 14 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 15 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 16 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 17 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 18 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 19 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 20 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 21 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 22 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 23 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 24 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 25 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 26 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 27 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 28 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 29 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 30 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 31 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 32 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 33 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 34 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 35 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 36 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 37 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 38 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 39 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n","/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(10, 40 + 1) instead\n","  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"]},{"output_type":"stream","name":"stdout","text":["   500/20000: episode: 1, duration: 5.141s, episode steps: 500, steps per second:  97, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.445011, mae: 49.085276, mean_q: 99.193228\n","  1000/20000: episode: 2, duration: 6.068s, episode steps: 500, steps per second:  82, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.603980, mae: 50.438328, mean_q: 101.127731\n","  1500/20000: episode: 3, duration: 5.561s, episode steps: 500, steps per second:  90, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.941162, mae: 50.738842, mean_q: 101.870049\n","  2000/20000: episode: 4, duration: 4.070s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.141252, mae: 50.754089, mean_q: 101.956482\n","  2500/20000: episode: 5, duration: 4.912s, episode steps: 500, steps per second: 102, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.103988, mae: 51.008392, mean_q: 102.403893\n","  3000/20000: episode: 6, duration: 4.090s, episode steps: 500, steps per second: 122, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.897403, mae: 51.826992, mean_q: 104.027023\n","  3500/20000: episode: 7, duration: 4.068s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.441112, mae: 52.614246, mean_q: 105.426231\n","  4000/20000: episode: 8, duration: 5.021s, episode steps: 500, steps per second: 100, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.021814, mae: 52.888870, mean_q: 105.884743\n","  4500/20000: episode: 9, duration: 4.076s, episode steps: 500, steps per second: 123, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.375700, mae: 53.321575, mean_q: 106.820114\n","  5000/20000: episode: 10, duration: 4.117s, episode steps: 500, steps per second: 121, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.127170, mae: 53.655289, mean_q: 107.572495\n","  5500/20000: episode: 11, duration: 5.118s, episode steps: 500, steps per second:  98, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.159072, mae: 54.108826, mean_q: 108.189095\n","  5588/20000: episode: 12, duration: 0.724s, episode steps:  88, steps per second: 122, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 11.945929, mae: 54.406113, mean_q: 108.759033\n","  6088/20000: episode: 13, duration: 4.184s, episode steps: 500, steps per second: 119, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.527770, mae: 54.538868, mean_q: 109.273674\n","  6588/20000: episode: 14, duration: 4.257s, episode steps: 500, steps per second: 117, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 13.328859, mae: 54.932442, mean_q: 110.009773\n","  7088/20000: episode: 15, duration: 5.057s, episode steps: 500, steps per second:  99, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.228662, mae: 55.049057, mean_q: 110.271782\n","  7588/20000: episode: 16, duration: 4.263s, episode steps: 500, steps per second: 117, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.975907, mae: 55.106243, mean_q: 110.443283\n","  8088/20000: episode: 17, duration: 4.922s, episode steps: 500, steps per second: 102, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.968428, mae: 55.066082, mean_q: 110.182747\n","  8588/20000: episode: 18, duration: 4.650s, episode steps: 500, steps per second: 108, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.492597, mae: 54.758549, mean_q: 109.490814\n","  9088/20000: episode: 19, duration: 4.276s, episode steps: 500, steps per second: 117, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.450494, mae: 54.495262, mean_q: 109.007904\n","  9588/20000: episode: 20, duration: 5.124s, episode steps: 500, steps per second:  98, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.621449, mae: 54.258320, mean_q: 108.519142\n"," 10088/20000: episode: 21, duration: 4.283s, episode steps: 500, steps per second: 117, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.340369, mae: 53.897060, mean_q: 107.905739\n"," 10588/20000: episode: 22, duration: 4.385s, episode steps: 500, steps per second: 114, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.586346, mae: 53.765617, mean_q: 107.426308\n"," 11088/20000: episode: 23, duration: 5.285s, episode steps: 500, steps per second:  95, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.487367, mae: 53.575558, mean_q: 107.096146\n"," 11588/20000: episode: 24, duration: 4.510s, episode steps: 500, steps per second: 111, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 8.776684, mae: 53.662987, mean_q: 107.151268\n"," 12088/20000: episode: 25, duration: 4.697s, episode steps: 500, steps per second: 106, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.872540, mae: 53.674381, mean_q: 107.160896\n"," 12345/20000: episode: 26, duration: 2.865s, episode steps: 257, steps per second:  90, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.617661, mae: 53.562675, mean_q: 106.867966\n"," 12361/20000: episode: 27, duration: 0.151s, episode steps:  16, steps per second: 106, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.760367, mae: 53.365860, mean_q: 106.699242\n"," 12861/20000: episode: 28, duration: 4.359s, episode steps: 500, steps per second: 115, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 11.846663, mae: 53.633289, mean_q: 106.906532\n"," 13148/20000: episode: 29, duration: 2.467s, episode steps: 287, steps per second: 116, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 13.354889, mae: 53.734726, mean_q: 107.301582\n"," 13299/20000: episode: 30, duration: 1.320s, episode steps: 151, steps per second: 114, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 11.232806, mae: 53.885963, mean_q: 107.733955\n"," 13456/20000: episode: 31, duration: 1.733s, episode steps: 157, steps per second:  91, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 14.111030, mae: 54.173435, mean_q: 108.231018\n"," 13579/20000: episode: 32, duration: 1.626s, episode steps: 123, steps per second:  76, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 7.181070, mae: 54.529942, mean_q: 109.382538\n"," 13664/20000: episode: 33, duration: 0.755s, episode steps:  85, steps per second: 113, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 20.040293, mae: 54.862679, mean_q: 109.607666\n"," 14164/20000: episode: 34, duration: 4.429s, episode steps: 500, steps per second: 113, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 18.438593, mae: 55.251953, mean_q: 110.477127\n"," 14581/20000: episode: 35, duration: 3.673s, episode steps: 417, steps per second: 114, episode reward: 417.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 17.636972, mae: 55.685757, mean_q: 111.519447\n"," 14696/20000: episode: 36, duration: 1.005s, episode steps: 115, steps per second: 114, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 13.471888, mae: 56.017555, mean_q: 112.371231\n"," 15035/20000: episode: 37, duration: 3.874s, episode steps: 339, steps per second:  88, episode reward: 339.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.499 [0.000, 1.000],  loss: 17.938602, mae: 56.346100, mean_q: 112.861519\n"," 15072/20000: episode: 38, duration: 0.337s, episode steps:  37, steps per second: 110, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 38.107498, mae: 56.679649, mean_q: 113.243202\n"," 15214/20000: episode: 39, duration: 1.242s, episode steps: 142, steps per second: 114, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 14.348967, mae: 56.643520, mean_q: 113.412338\n"," 15255/20000: episode: 40, duration: 0.362s, episode steps:  41, steps per second: 113, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 15.069443, mae: 56.882442, mean_q: 114.054131\n"," 15372/20000: episode: 41, duration: 1.019s, episode steps: 117, steps per second: 115, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7.274410, mae: 56.785175, mean_q: 114.206879\n"," 15475/20000: episode: 42, duration: 0.904s, episode steps: 103, steps per second: 114, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 22.985960, mae: 57.130856, mean_q: 114.348991\n"," 15619/20000: episode: 43, duration: 1.264s, episode steps: 144, steps per second: 114, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 23.013235, mae: 57.336636, mean_q: 114.598930\n"," 15738/20000: episode: 44, duration: 1.049s, episode steps: 119, steps per second: 113, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 14.280368, mae: 57.361198, mean_q: 114.843750\n"," 16238/20000: episode: 45, duration: 5.336s, episode steps: 500, steps per second:  94, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.515652, mae: 57.282337, mean_q: 114.611572\n"," 16296/20000: episode: 46, duration: 0.530s, episode steps:  58, steps per second: 109, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 19.058552, mae: 57.379173, mean_q: 114.792412\n"," 16415/20000: episode: 47, duration: 1.067s, episode steps: 119, steps per second: 112, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.445 [0.000, 1.000],  loss: 16.339077, mae: 57.695637, mean_q: 115.602272\n"," 16573/20000: episode: 48, duration: 1.388s, episode steps: 158, steps per second: 114, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 19.483492, mae: 57.607132, mean_q: 115.095245\n"," 16635/20000: episode: 49, duration: 0.570s, episode steps:  62, steps per second: 109, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 16.494558, mae: 57.396294, mean_q: 115.081032\n"," 16688/20000: episode: 50, duration: 0.463s, episode steps:  53, steps per second: 114, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 21.087412, mae: 57.813068, mean_q: 115.769348\n"," 16851/20000: episode: 51, duration: 1.484s, episode steps: 163, steps per second: 110, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 8.995768, mae: 57.846302, mean_q: 116.041939\n"," 16932/20000: episode: 52, duration: 0.763s, episode steps:  81, steps per second: 106, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 15.669153, mae: 57.616699, mean_q: 115.488861\n"," 17092/20000: episode: 53, duration: 1.524s, episode steps: 160, steps per second: 105, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 21.278046, mae: 57.810265, mean_q: 115.633057\n"," 17239/20000: episode: 54, duration: 1.345s, episode steps: 147, steps per second: 109, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 14.138338, mae: 57.893063, mean_q: 115.874641\n"," 17394/20000: episode: 55, duration: 1.728s, episode steps: 155, steps per second:  90, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 16.351244, mae: 57.786057, mean_q: 115.513580\n"," 17494/20000: episode: 56, duration: 1.357s, episode steps: 100, steps per second:  74, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 6.232143, mae: 57.921722, mean_q: 116.137764\n"," 17600/20000: episode: 57, duration: 1.090s, episode steps: 106, steps per second:  97, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 19.455589, mae: 57.983250, mean_q: 115.850456\n"," 17727/20000: episode: 58, duration: 1.121s, episode steps: 127, steps per second: 113, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 19.190935, mae: 57.926495, mean_q: 115.770538\n"," 17873/20000: episode: 59, duration: 1.306s, episode steps: 146, steps per second: 112, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 20.386562, mae: 58.014500, mean_q: 115.768219\n"," 18005/20000: episode: 60, duration: 1.199s, episode steps: 132, steps per second: 110, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 21.251287, mae: 57.745266, mean_q: 115.228882\n"," 18052/20000: episode: 61, duration: 0.439s, episode steps:  47, steps per second: 107, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 19.340538, mae: 57.774860, mean_q: 115.545670\n"," 18142/20000: episode: 62, duration: 0.812s, episode steps:  90, steps per second: 111, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 21.863289, mae: 57.757618, mean_q: 115.268692\n"," 18258/20000: episode: 63, duration: 1.046s, episode steps: 116, steps per second: 111, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 20.420994, mae: 58.260101, mean_q: 116.168159\n"," 18475/20000: episode: 64, duration: 1.951s, episode steps: 217, steps per second: 111, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 16.789001, mae: 57.641869, mean_q: 115.187500\n"," 18609/20000: episode: 65, duration: 1.236s, episode steps: 134, steps per second: 108, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 21.982265, mae: 57.730179, mean_q: 115.123108\n"," 18871/20000: episode: 66, duration: 3.236s, episode steps: 262, steps per second:  81, episode reward: 262.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 16.856058, mae: 57.589657, mean_q: 115.027695\n"," 18975/20000: episode: 67, duration: 0.928s, episode steps: 104, steps per second: 112, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 17.570026, mae: 57.761395, mean_q: 115.435074\n"," 19226/20000: episode: 68, duration: 2.228s, episode steps: 251, steps per second: 113, episode reward: 251.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 13.923415, mae: 57.528595, mean_q: 115.140862\n"," 19614/20000: episode: 69, duration: 3.517s, episode steps: 388, steps per second: 110, episode reward: 388.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 17.991785, mae: 57.484474, mean_q: 114.921333\n"," 19980/20000: episode: 70, duration: 3.821s, episode steps: 366, steps per second:  96, episode reward: 366.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 17.647528, mae: 57.195034, mean_q: 114.412598\n","done, took 189.157 seconds\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-f352772af7eb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_repetition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_max_start_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_step_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_max_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps_warmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menable_double_dqn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_dueling_network\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdueling_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_model_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"]}],"source":["policy = EpsGreedyQPolicy()\n","memory = SequentialMemory(limit=50000, window_length=10) # window_length phải bằng input_shape ở trên nhé\n","dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n","dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n","dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)\n","fit(self, env, nb_steps, action_repetition=1, callbacks=None, verbose=1, visualize=False, nb_max_start_steps=0, start_step_policy=None, log_interval=10000, nb_max_episode_steps=None)\n","\n","agent = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=16,enable_double_dqn=True, enable_dueling_network=True, dueling_type='avg',target_model_update=1e-2, policy=policy, batch_size=16)"]},{"cell_type":"markdown","metadata":{"id":"hqj5CMWb3DI3"},"source":["## test"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jG36gqLGwr87","executionInfo":{"status":"ok","timestamp":1682443883217,"user_tz":-420,"elapsed":49040,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"ea2c1813-d04a-425e-9156-8d8140e20086"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing for 100 episodes ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1: reward: 500.000, steps: 500\n","Episode 2: reward: 500.000, steps: 500\n","Episode 3: reward: 500.000, steps: 500\n","Episode 4: reward: 500.000, steps: 500\n","Episode 5: reward: 500.000, steps: 500\n","Episode 6: reward: 500.000, steps: 500\n","Episode 7: reward: 500.000, steps: 500\n","Episode 8: reward: 500.000, steps: 500\n","Episode 9: reward: 500.000, steps: 500\n","Episode 10: reward: 500.000, steps: 500\n","Episode 11: reward: 500.000, steps: 500\n","Episode 12: reward: 500.000, steps: 500\n","Episode 13: reward: 500.000, steps: 500\n","Episode 14: reward: 500.000, steps: 500\n","Episode 15: reward: 500.000, steps: 500\n","Episode 16: reward: 500.000, steps: 500\n","Episode 17: reward: 500.000, steps: 500\n","Episode 18: reward: 500.000, steps: 500\n","Episode 19: reward: 500.000, steps: 500\n","Episode 20: reward: 500.000, steps: 500\n","Episode 21: reward: 500.000, steps: 500\n","Episode 22: reward: 500.000, steps: 500\n","Episode 23: reward: 500.000, steps: 500\n","Episode 24: reward: 500.000, steps: 500\n","Episode 25: reward: 500.000, steps: 500\n","Episode 26: reward: 500.000, steps: 500\n","Episode 27: reward: 500.000, steps: 500\n","Episode 28: reward: 500.000, steps: 500\n","Episode 29: reward: 500.000, steps: 500\n","Episode 30: reward: 500.000, steps: 500\n","Episode 31: reward: 500.000, steps: 500\n","Episode 32: reward: 500.000, steps: 500\n","Episode 33: reward: 500.000, steps: 500\n","Episode 34: reward: 500.000, steps: 500\n","Episode 35: reward: 500.000, steps: 500\n","Episode 36: reward: 500.000, steps: 500\n","Episode 37: reward: 500.000, steps: 500\n","Episode 38: reward: 500.000, steps: 500\n","Episode 39: reward: 500.000, steps: 500\n","Episode 40: reward: 500.000, steps: 500\n","Episode 41: reward: 500.000, steps: 500\n","Episode 42: reward: 500.000, steps: 500\n","Episode 43: reward: 500.000, steps: 500\n","Episode 44: reward: 500.000, steps: 500\n","Episode 45: reward: 500.000, steps: 500\n","Episode 46: reward: 500.000, steps: 500\n","Episode 47: reward: 500.000, steps: 500\n","Episode 48: reward: 500.000, steps: 500\n","Episode 49: reward: 500.000, steps: 500\n","Episode 50: reward: 500.000, steps: 500\n","Episode 51: reward: 500.000, steps: 500\n","Episode 52: reward: 500.000, steps: 500\n","Episode 53: reward: 500.000, steps: 500\n","Episode 54: reward: 500.000, steps: 500\n","Episode 55: reward: 500.000, steps: 500\n","Episode 56: reward: 500.000, steps: 500\n","Episode 57: reward: 500.000, steps: 500\n","Episode 58: reward: 500.000, steps: 500\n","Episode 59: reward: 500.000, steps: 500\n","Episode 60: reward: 500.000, steps: 500\n","Episode 61: reward: 500.000, steps: 500\n","Episode 62: reward: 500.000, steps: 500\n","Episode 63: reward: 500.000, steps: 500\n","Episode 64: reward: 500.000, steps: 500\n","Episode 65: reward: 500.000, steps: 500\n","Episode 66: reward: 500.000, steps: 500\n","Episode 67: reward: 500.000, steps: 500\n","Episode 68: reward: 500.000, steps: 500\n","Episode 69: reward: 500.000, steps: 500\n","Episode 70: reward: 500.000, steps: 500\n","Episode 71: reward: 500.000, steps: 500\n","Episode 72: reward: 500.000, steps: 500\n","Episode 73: reward: 500.000, steps: 500\n","Episode 74: reward: 500.000, steps: 500\n","Episode 75: reward: 500.000, steps: 500\n","Episode 76: reward: 500.000, steps: 500\n","Episode 77: reward: 500.000, steps: 500\n","Episode 78: reward: 500.000, steps: 500\n","Episode 79: reward: 500.000, steps: 500\n","Episode 80: reward: 500.000, steps: 500\n","Episode 81: reward: 500.000, steps: 500\n","Episode 82: reward: 500.000, steps: 500\n","Episode 83: reward: 500.000, steps: 500\n","Episode 84: reward: 500.000, steps: 500\n","Episode 85: reward: 500.000, steps: 500\n","Episode 86: reward: 500.000, steps: 500\n","Episode 87: reward: 500.000, steps: 500\n","Episode 88: reward: 500.000, steps: 500\n","Episode 89: reward: 500.000, steps: 500\n","Episode 90: reward: 500.000, steps: 500\n","Episode 91: reward: 500.000, steps: 500\n","Episode 92: reward: 500.000, steps: 500\n","Episode 93: reward: 500.000, steps: 500\n","Episode 94: reward: 500.000, steps: 500\n","Episode 95: reward: 500.000, steps: 500\n","Episode 96: reward: 500.000, steps: 500\n","Episode 97: reward: 500.000, steps: 500\n","Episode 98: reward: 500.000, steps: 500\n","Episode 99: reward: 500.000, steps: 500\n","Episode 100: reward: 500.000, steps: 500\n","Average reward over 100 episodes: 500.0\n"]}],"source":["his = dqn.test(env, nb_episodes=100, visualize=False).history['episode_reward']\n","print(\"Average reward over\", len(his), \"episodes:\", np.sum(his)/len(his))"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNO33AZ4NMcR8J87AT8JFkj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}