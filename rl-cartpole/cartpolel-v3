{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtqr0GncZLo+SBfRetQFk+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"5GC5FLGngVWA","executionInfo":{"status":"ok","timestamp":1682360891326,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"outputs":[],"source":["import gym\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow as tf"]},{"cell_type":"code","source":["# Define custom environment\n","class CustomEnv(gym.Env):\n","    def __init__(self):\n","        # Define environment parameters\n","        self.env = gym.make('CartPole-v1')\n","        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(100, 100, 3))\n","        self.action_space = self.env.action_space\n","\n","\n","    def reset(self):\n","        # Reset environment state\n","        return self.env.reset()\n","\n","    def step(self, action):\n","        # Take action and return next state, reward, done flag, and info\n","        observation, reward, done, info = self.env.step(action)\n","        observation = self._preprocess_observation(observation)\n","        return observation, reward, done, info\n","\n","    def _preprocess_observation(self, observation):\n","        # Convert continuous state values to image\n","        cart_x = int(observation[0] * 50 + 50)\n","        pole_x = int(observation[2] * 50 + cart_x)\n","        img = np.zeros((100, 100, 3))\n","        img[:, :, 1] = 255\n","        img[95:100, cart_x-10:cart_x+10, :] = 0\n","        img[50:55, pole_x-1:pole_x+1, :] = [255, 0, 0]\n","        return img.astype(np.uint8)\n","\n","    def render(self, mode='human'):\n","        # Render environment\n","        ...\n"],"metadata":{"id":"Qx9ja3dHhCFk","executionInfo":{"status":"ok","timestamp":1682360833537,"user_tz":-420,"elapsed":316,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class CNNModel(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n","        self.pool1 = tf.keras.layers.MaxPooling2D(2)\n","        self.conv2 = tf.keras.layers.Conv2D(64, 3, activation='relu')\n","        self.pool2 = tf.keras.layers.MaxPooling2D(2)\n","        self.flatten = tf.keras.layers.Flatten()\n","\n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.pool1(x)\n","        x = self.conv2(x)\n","        x = self.pool2(x)\n","        x = self.flatten(x)\n","        return x"],"metadata":{"id":"0O8GUlqdhbIi","executionInfo":{"status":"ok","timestamp":1682360892973,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class QTable:\n","    def __init__(self, n_actions, n_features):\n","        self.n_actions = n_actions\n","        self.n_features = n_features\n","        self.q_table = np.zeros((n_features, n_actions))\n","\n","    def choose_action(self, observation, eps):\n","        if np.random.uniform() < eps:\n","            return np.random.choice(self.n_actions)\n","        else:\n","            cnn_model = CNNModel()\n","            features = cnn_model(np.expand_dims(observation, axis=0))\n","            q"],"metadata":{"id":"znXXiH1-hTUE","executionInfo":{"status":"ok","timestamp":1682361077602,"user_tz":-420,"elapsed":317,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters.\n","num_episodes = 1000\n","epsilon = 0.1\n","# epsilon = 1.0\n","batch_size = 32\n","discount = 0.99\n","# buffer = ReplayBuffer(100000)\n","cur_frame = 0\n","num_timesteps = 1000\n","learning_rate = 0.8\n","discount_factor = 0.95"],"metadata":{"id":"aTtkToL3jBgD","executionInfo":{"status":"ok","timestamp":1682361412600,"user_tz":-420,"elapsed":336,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Create environment\n","env = CustomEnv()\n","\n","# Initialize Q-table\n","# q_table = QTable(np.zeros([env.observation_space.n, env.action_space.n]))\n","# q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","obs_space_shape = env.observation_space.shape\n","action_space_size = env.action_space.n\n","q_table = np.zeros((obs_space_shape[0], action_space_size))\n","\n","# Define CNN model\n","model = Sequential()\n","model.add(Conv2D(32, (4,4), activation='relu', input_shape=env.observation_space.shape))\n","model.add(Conv2D(64, (2,2), activation='relu'))\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(env.action_space.n, activation='linear'))\n","\n","# Compile model\n","model.compile(loss='mse', optimizer=Adam())\n","\n","# Train agent using Q-learning\n","num_episodes = 1000\n","for episode in range(num_episodes):\n","    state = env.reset()\n","    # state = state.astype('float32')\n","\n","    print(\"state:\", state)\n","\n","    done = False\n","\n","    while not done:\n","        # Choose action using epsilon-greedy policy\n","        if np.random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            # action = np.argmax(q_table[int(state.item()), :])\n","            action = np.argmax(q_table[int(state.item()), :])\n","\n","        # Take action and observe next state, reward, done flag, and info\n","        next_state, reward, done, info = env.step(action)\n","\n","        # Update Q-table using Bellman equation\n","        q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n","            learning_rate * (reward + discount_factor * np.max(q_table[next_state, :]))\n","\n","        state = next_state\n","\n","# # Test agent by running episodes of environment\n","# for episode in range(num_episodes):\n","#     state = env.reset()\n","#     print(\"state:\"+ state)\n","#     done = False\n","\n","#     while not done:\n","#         # Choose action using learned Q-values\n","#         # action = np.argmax(q_table[int(state.item()), :])\n","#         action = np.argmax(q_table[int(state.item()), :])\n","\n","#         # action = select_epsilon_greedy_action(state_in, epsilon)\n","\n","\n","#         # Take action and observe next state, reward, done flag, and info\n","#         next_state, reward, done, info = env.step(action)\n","\n","#         state = next_state\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"id":"fME6uGFugX_2","executionInfo":{"status":"error","timestamp":1682362438486,"user_tz":-420,"elapsed":4004,"user":{"displayName":"Hà Hải","userId":"02289900666155783825"}},"outputId":"d17d446b-fd74-4e53-8283-8267f44f3915"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["state: [0.00713325 0.03937495 0.03130504 0.03663436]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-2cef7dc9b48d>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# action = np.argmax(q_table[int(state.item()), :])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Take action and observe next state, reward, done flag, and info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"]}]}]}